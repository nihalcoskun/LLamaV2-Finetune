# **About the Project**

This project was created during my internship at the Digital Transformation Office from 31.07.2023 to 11.09.2023. 
The main goal of our project is to fine-tune the LLamaV2 model to effectively work in the Turkish language. 
This process was developed focusing on machine learning and natural language processing (NLP) techniques using Python.



## **Development of the Project**


**LLamaV2 Model Fine-Tuning and Language Adaptation**


**LLamaV2 Model:** After completing focused fine-tuning studies in English, I transitioned to the LLamaV2 model, 
which was the main target for providing Turkish language support. Initially, LLamaV2 was an open-source model that did not support Turkish.


**Data Sets and Formats:** To train the model in the Turkish language, multiple Turkish datasets were used from Huggingface. 
We worked with datasets in many different formats and made inferences based on the training results. 
Finally, the Turkish Alpaca dataset was processed in a way that the model could understand, and fine-tuning was performed.


**Coding and Customization Work:** Necessary coding and customization work in Python was carried out for the model to effectively work in Turkish.
This process required understanding the language structure and syntax.


**Finetuning Process:** The finetuning process was a pivotal aspect of optimizing the LLamaV2 model. Specifically, the Llama-2-13B-Chat-fp16 model
was trained using the afkfatih/turkishdataset corpus. This training was conducted in a hardware environment supported by CUDA with 2 GPUs,
ensuring efficient and robust processing capabilities. The model underwent an extensive training regime over 50 epochs, which allowed for 
significant adjustments and improvements in its ability to understand and process the Turkish language accurately. This rigorous finetuning
process was crucial in enhancing the model's performance, making it more adept at handling the complexities of natural 
language processing in Turkish.


## **Model Optimization and Testing Processes**


**Hyperparameter Tuning:** The tuning of hyperparameters was carried out to maximize the performance of the model. 
This was critical to enhance the efficiency and accuracy of the model's training process.


**Data Set Preparation and Formatting:** Preparing and formatting the data sets in a suitable format for the model was important for the success
of the training process. This included proper tokenization and data structuring.


**Tokenization Processes:** Tokenization processes were implemented for the model to effectively process the data. 
This was a critical step in enabling the language model to understand and process text.


**Testing and Evaluation Stages:** Various testing stages were conducted to evaluate and improve the model's performance.
This process involved steps such as loss value analysis, evaluating overfitting and underfitting situations, and analyzing
the model's ability to create context as a language model.


## **Model CheckpÄ±int Tests**

<img width="454" alt="image" src="https://github.com/nihalcoskun/LLamaV2-Finetune/assets/77547456/730f78e8-483f-44d5-a6c3-f1f381fffe99">


<img width="454" alt="image" src="https://github.com/nihalcoskun/LLamaV2-Finetune/assets/77547456/12cd9117-c4dd-4163-ae4d-6b830f632b1e">


<img width="454" alt="image" src="https://github.com/nihalcoskun/LLamaV2-Finetune/assets/77547456/c18707cd-baec-41b5-bd4b-8a376ea0737a">


<img width="454" alt="image" src="https://github.com/nihalcoskun/LLamaV2-Finetune/assets/77547456/ec32f297-9473-4c49-8c77-768575305d01">


<img width="454" alt="image" src="https://github.com/nihalcoskun/LLamaV2-Finetune/assets/77547456/b51afbc6-e106-4dee-a610-33fb9f251ceb">

<img width="454" alt="image" src="https://github.com/nihalcoskun/LLamaV2-Finetune/assets/77547456/830e2dec-8828-4fcc-b9f6-f6adb4a70440">


<img width="454" alt="image" src="https://github.com/nihalcoskun/LLamaV2-Finetune/assets/77547456/600139a5-7678-462f-89d8-84b118b09556">
